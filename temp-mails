#!/usr/bin/env python3

# Copyright (c) 2023-present the-weird-aquarian

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.


import requests
from bs4 import BeautifulSoup


def get_url_response(url):
    try:
        return requests.get(url)
    except requests.exceptions as exception:
        print(f"Failed to connect to {url}: {exception}\nExiting script.\n")
        exit()


def scrape_mailcheck_site(div):
    domains_list = []
    
    # Extract domain names
    for tag in div.find_all("a"):
        href = tag.get("href")
        if href and f"{base_mailcheck_url}/domain" in href:
            domain = href.split("/")[-1]
            domains_list.append(domain)

    return domains_list


github_data_urls = [
    "https://raw.githubusercontent.com/7c/fakefilter/main/txt/data.txt",
    "https://raw.githubusercontent.com/disposable-email-domains/disposable-email-domains/master/disposable_email_blocklist.conf",
    "https://raw.githubusercontent.com/wesbos/burner-email-providers/master/emails.txt",
    "https://raw.githubusercontent.com/stopforumspam/disposable_email_domains/master/blacklist.txt",
    "https://raw.githubusercontent.com/FGRibreau/mailchecker/master/list.txt"
    ]

github_display_urls = [
    "https://github.com/7c/fakefilter",
    "https://github.com/disposable-email-domains/disposable-email-domains",
    "https://github.com/wesbos/burner-email-providers",
    "https://github.com/stopforumspam/disposable_email_domains",
    "https://github.com/FGRibreau/mailchecker"
    ]

base_mailcheck_url = "https://www.mailcheck.ai"
extras_file = "extras.txt"
output_file = "final_list.txt"

github_response_data = []
print("")
for data_url, display_url in zip(github_data_urls, github_display_urls):
    print(f"[+] Retrieving data from {display_url}\n")
    response = get_url_response(data_url)
    github_response_data.append(response.text.splitlines())

print(f"[+] Retrieving data from extras.txt\n")
with open(extras_file, "r") as extras:
    extras_data = extras.read().splitlines()

print(f"[+] Retrieving data from {base_mailcheck_url}")
print(f"This will definitely take some time ...")
mailcheck_data = []
# Iterate from a-z and 0-9
for char in list(map(chr, range(ord("a"), ord("z") + 1))) + list(map(str, range(10))):
    print(f"  > Checking domains with \"{char}\"")
    page_number = 1
    while True:
        mailcheck_url = f"{base_mailcheck_url}/directory/{char}?page={page_number}"
        soup = BeautifulSoup(requests.get(mailcheck_url).text, "html.parser")
        div = soup.find("div", class_="grid gap-10 mt-6 lg:grid-cols-5 md:grid-col-3 sm:grid-cols-2")
        if div and div.find("a"):
            print(f"    Page {page_number}")
            mailcheck_data.extend(scrape_mailcheck_site(div))
            page_number += 1
        else:
            break

print(f"\n>> Combining data ...\n")
github_data = []
for github_response_data_lines in github_response_data:
    # Remove empty lines & lines starting with "#"
    github_data.append([line for line in github_response_data_lines if line.strip() and not line.startswith("#")])

combined_data = github_data + extras_data + mailcheck_data
print(f">> Removing duplicates ...\n")
combined_data = list(set(combined_data))
print(f">> Sorting list ...\n")
combined_data.sort()

with open(output_file, "w") as file:
    print(f">> Writing output to {output_file} ...\n")
    for item in combined_data:
        file.write(f"{item}\n")
    print("Done.\n")