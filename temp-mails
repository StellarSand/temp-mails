#!/usr/bin/env python3

# Copyright (c) 2023-present the-weird-aquarian

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.


import requests
from bs4 import BeautifulSoup
from random import randint
import time


def get_url_response(url):
    connection_timeout = 10
    data_read_timeout = 20
    # If required, check headers from https://myhttpheader.com/ or https://httpbin.org/headers
    # This should help to avoid some of scraping detection by pretending to be real browser
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; rv:109.0) Gecko/20100101 Firefox/115.0",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Connection": "keep-alive",
        "Cache-Control": "no-cache",
        "DNT": "1",
        "Pragma": "no-cache",
        "Upgrade-Insecure-Requests": "1",
        "X-Requested-With": "XMLHttpRequest"
        }

    try:
        return requests.get(url, timeout=(connection_timeout, data_read_timeout), headers=headers)
    except requests.exceptions.RequestException as exception:
        print(f"Failed to connect to {url}: {exception}\nExiting script.\n")
        exit()


def scrape_mailcheck_site(div):
    domains_list = []
    
    # Extract domain names
    for tag in div.find_all("a"):
        href = tag.get("href")
        if href and f"{base_mailcheck_url}/domain" in href:
            domain = href.split("/")[-1]
            domains_list.append(domain)

    return domains_list


github_data_urls = [
    "https://raw.githubusercontent.com/7c/fakefilter/main/txt/data.txt",
    "https://raw.githubusercontent.com/disposable-email-domains/disposable-email-domains/master/disposable_email_blocklist.conf",
    "https://raw.githubusercontent.com/wesbos/burner-email-providers/master/emails.txt",
    "https://raw.githubusercontent.com/stopforumspam/disposable_email_domains/master/blacklist.txt",
    "https://raw.githubusercontent.com/FGRibreau/mailchecker/master/list.txt"
    ]

github_display_urls = [
    "https://github.com/7c/fakefilter",
    "https://github.com/disposable-email-domains/disposable-email-domains",
    "https://github.com/wesbos/burner-email-providers",
    "https://github.com/stopforumspam/disposable_email_domains",
    "https://github.com/FGRibreau/mailchecker"
    ]

# Exclude Anonaddy & SimpleLogin domains
allowed_domains = [
    "addy.io",
    "aleeas.com",
    "anonaddy.com",
    "anonaddy.me",
    "silomails.com",
    "simplelogin.co",
    "simplelogin.fr",
    "simplelogin.io",
    "slmail.me",
    "slmails.com"
    ]

base_mailcheck_url = "https://www.mailcheck.ai"
extras_file = "extras.txt"
output_file = "final_list.txt"

github_response_data = []
print("")
for display_url, data_url in zip(github_display_urls, github_data_urls):
    print(f"[+] Retrieving data from {display_url}\n")
    response = get_url_response(data_url)
    github_response_data.extend(response.text.splitlines())

print(f"[+] Retrieving data from extras.txt\n")
with open(extras_file, "r") as extras:
    extras_data = extras.read().splitlines()

print(f"[+] Retrieving data from {base_mailcheck_url}")
print(f"Sit back & relax. This will definitely take some time ...")
mailcheck_data = []
# Iterate from a-z and 0-9
for char in list(map(chr, range(ord("a"), ord("z") + 1))) + list(map(str, range(10))):
    print(f"  > Checking domains with \"{char}\"")
    page_number = 1
    while True:
        mailcheck_url = f"{base_mailcheck_url}/directory/{char}?page={page_number}"
        soup = BeautifulSoup(get_url_response(mailcheck_url).text, "html.parser")
        div = soup.find("div", class_="grid gap-10 mt-6 lg:grid-cols-5 md:grid-col-3 sm:grid-cols-2")
        if div and div.find("a"):
            print(f"    Page {page_number}")
            mailcheck_data.extend(scrape_mailcheck_site(div))
            page_number += 1
            delay = randint(3, 10) # Generate a random delay between 3 and 10 seconds
            print(f"    --- Pausing for {delay} seconds ---")
            time.sleep(delay) # This should help to avoid some of scraping detection by the website
        else:
            break

print(f"\n>> Combining data ...\n")
combined_data = []
for line in github_response_data:
    # Remove empty lines & lines starting with "#"
    if line.strip() and not line.startswith("#"):
        combined_data.append(line)

combined_data.extend(extras_data)
combined_data.extend(mailcheck_data)
combined_data = list(set(combined_data)) # Remove duplicates
combined_data.sort()

with open(output_file, "w") as file:
    for item in combined_data:
        if item not in allowed_domains:
            file.write(f"{item}\n")
    print(f">> Output written to {output_file}.\n")
